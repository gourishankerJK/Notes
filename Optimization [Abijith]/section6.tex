\section{Proximal Methods}

\subsection{Projected Subgradient Descent}

\begin{algorithm}[H]
    \KwIn{Initialisation $x^{(0)}\in C$, projection operator $\Pi_C$ onto the constrain set $C$}
    \KwOut{Minimiser $\displaystyle x^* = \arg\min_{x\in C} f(x)$}
    \For{$k=1,2,\ldots,$ until convergence}{
        Choose a suitable step-size $t_k > 0$ \;
        $z_{k+1} = x_{k} - t_k \zeta_k$, $\zeta_k\in\partial f(x_k)$ \;
        $x_{k+1} = \Pi_C(z_{k+1})$
    }
    \Return $x^{(k+1)}$
    \caption{Projected subgradient descent for constrained minimisation}
\end{algorithm}

{\bf Assumptions:}
\begin{enumerate}
    \item $f:X\rightarrow\bar{\bb R}$ is proper, closed and $\sigma$-strongly convex,
    \item $C\subseteq\text{int}(\text{dom}\{f\})$,
    \item $f^* = \min_C f > -\infty, \arg\min_C f$ is nonempty,
    \item $\exists L>0: \forall x\in C, \zeta\in\partial f(x), \norm{\zeta}\leq L$.
\end{enumerate}

\begin{example}
    
\end{example}

\begin{theorem}
    Let $(x_k)$ be the sequence generated by $x_{k+1} = \Pi_C(x_k-t_k\zeta_k)$, where $\zeta\in\partial f(x_k), t_k = \frac{2}{\sigma(k+1)}$. Then,
    \[
        \underbrace{\min_{1\leq k\leq n} f(x_k)}_{f_n^*} - f^* = o(1/n).
    \]
    Moreover, if $i_n$ such that $f(x_{i_n}) = f^*_n$, then $\norm{x_{i_n}-x^*}^2 = o(1/n)$.
\end{theorem}

%% -----------------------------------------------------------
%   LECTURE 18
%% -----------------------------------------------------------

\subsection{Proximal Gradient Method}

\begin{definition}[Proximal Operator]
    Let $f\in\Gamma_0(X)$. The proximal operator of $f$ is defined as
    \[
          \text{prox}_f:X\rightarrow X, \; \text{prox}_f(x) = \arg\min_{\theta\in X}\frac{1}{2}\norm{\theta-x}^2 + f(\theta).
    \]
\end{definition}

\begin{proposition}
    Let $f\in\Gamma_0(X)$. Then, for any $x\in X$, the function
    \[
        \theta \mapsto \frac{1}{2}\norm{\theta-x}^2 + f(\theta)
    \]
    is strongly convex. In particular, $\text{prox}_f$ is well-defined.
\end{proposition}
\begin{proof}
    The proof is easy if $\text{dom}\{f\}=X$. Else, we need the following observation.
    \begin{lemma}[Test 2 problem]
        Let $f\in\Gamma_0(X)$. Then, $\exists\zeta\in X,\alpha\in\bb R$ such that
        \[
            \forall x\in X: f(x) \geq \langle\zeta,x\rangle + \alpha.
        \]
    \end{lemma}
    {\color{red} finish this}
\end{proof}

\begin{theorem}
    Let $f\in\Gamma_0(X)$. Then, $\text{prox}_f$ is firmly-nonexpansive, i.e.,
    \begin{align*}
        \forall x,y\in X :& \norm{\text{prox}_f(x)-\text{prox}_f(y)}^2 + \norm{(x-\text{prox}_f(x))-(y-\text{prox}_f(y))}^2 \leq \norm{x-y}^2,\\
        \Leftrightarrow & \norm{\text{prox}_f(x)-\text{prox}_f(y)}^2 \leq \langle x-y,\text{prox}_f(x)-\text{prox}_f(y)\rangle.
    \end{align*}
\end{theorem}
\begin{proof}
    
\end{proof}

\begin{algorithm}[H]
    \KwIn{Initialisation $x_{0}\in X$, proximal operator $\text{prox}_g$}
    \KwOut{Minimiser $\displaystyle x^* = \arg\min_{x\in X} f(x) + g(x)$}
    \For{$k=1,2,\ldots,$ until convergence}{
        $z_{k+1} = x_{k} - \frac{1}{\beta}\nabla f(x_k)$ \;
        $x_{k+1} = \text{prox}_{\frac{1}{\beta}g}(z_{k+1})$
    }
    \Return $x^{(k+1)}$
    \caption{Proximal gradient method for nonsmooth optimisation}
\end{algorithm}
This subsumes everything till date
\begin{enumerate}
    \item Gradient descent $g=0$
    \item Projected gradient descent $g=\iota_C$
    \item* Proximal point algorithm $f=0$, $x_{k+1} = \text{prox}_{\rho g}(x_k), \;\rho>0$.
\end{enumerate}

{\it \noindent Q: Is there a connection between proximal operators and convex optimisation?\\
\noindent A: \ldots}

\begin{proposition}
    Let $\psi\in\Gamma_0(X)$ and $c>0$. Then,
    \[
        x^* = \arg\min_X\psi \Leftrightarrow x^*=\text{prox}_{c\psi}(x^*).
    \]
\end{proposition}

\begin{lemma}
    Let $f$ be convex and $\beta$-smooth, and let $g\in\Gamma_0(X)$. Define $T:X\rightarrow X$ as follows:
    \[
        T(x) = \text{prox}_{\beta^{-1}g}(x-\beta^{-1}\nabla f(x)).
    \]
    Then,
    \begin{enumerate}
        \item $x^*=\arg\min_X(f+g)\Leftrightarrow x^*\in\Fix\{T\}$
        \item $T = \text{prox}_\psi$ for some $\psi\in\Gamma_0(X)$. In particular, $T$ is firmly-nonexpansive, and the fixed point iterations $x_{k+1}=T(x_k)$, which is also the proximal gradient method, is convergent.
    \end{enumerate}
\end{lemma}